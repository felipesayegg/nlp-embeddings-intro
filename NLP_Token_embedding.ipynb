{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TOKENIZAÇÃO"
      ],
      "metadata": {
        "id": "Zhiw1z5nRt35"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW0kmMaIROb1"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer # ## IMPORTAÇÃO DSA BIBLIOTECAS\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "# --------------------------------------------\n",
        "# RESUMO DO QUE ACONTECE AQUI:\n",
        "#\n",
        "# GPT2Tokenizer.from_pretrained(\"gpt2\"):\n",
        "# 1) Procura no cache local do Hugging Face se já existe o tokenizador do modelo \"gpt2\".\n",
        "# 2) Se não existir, baixa os arquivos necessários do Hugging Face Hub:\n",
        "#    - vocab.json       → lista de tokens (subpalavras) e seus IDs numéricos\n",
        "#    - merges.txt       → regras de junção de subpalavras (BPE - Byte Pair Encoding)\n",
        "#    - tokenizer.json   → versão compacta com vocabulário + merges\n",
        "#    - tokenizer_config.json → configurações do tokenizador\n",
        "#    - config.json      → parâmetros do modelo (número de camadas, dimensões etc.)\n",
        "# 3) Carrega esses arquivos em memória e deixa o tokenizador pronto para uso.\n",
        "#\n",
        "# O argumento \"gpt2\" é o nome do modelo no repositório do Hugging Face.\n",
        "# Poderia ser trocado por outros modelos compatíveis, ex:\n",
        "#    GPT2Tokenizer.from_pretrained(\"distilgpt2\")   # versão reduzida\n",
        "#    GPT2Tokenizer.from_pretrained(\"gpt2-medium\") # versão maior\n",
        "#\n",
        "# Obs: Esse passo NÃO treina nada, apenas carrega um modelo pré-treinado.\n",
        "# --------------------------------------------\n"
      ],
      "metadata": {
        "id": "3OBwUYu6SDvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INSIRA SEU TEXTO ABAIXO EM FORMATO DE STRING\n",
        "text = \"SOU UM ALUNO DA FIAP E ADORO APRENDER A USAR INTELIGÊNCIA ARTIFICIAL\"\n"
      ],
      "metadata": {
        "id": "_EIJxWG-Tx2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palavra = text.split()\n",
        "palavra"
      ],
      "metadata": {
        "id": "XRJEHhomUmmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantidade_palavras = len(palavra)\n",
        "quantidade_palavras"
      ],
      "metadata": {
        "id": "h-Vfa0uDUmo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "# tokenizer.encode(text, add_special_tokens=True):\n",
        "# - Usa o tokenizador pré-treinado (GPT-2) para:\n",
        "#   1) Quebrar o texto em tokens (subpalavras)\n",
        "#   2) Converter cada token em um número (ID)\n",
        "# - text → string que quero transformar em IDs\n",
        "# - add_special_tokens=True → adiciona tokens especiais usados pelo modelo\n",
        "#   (ex: [CLS], [SEP] no BERT ou  no GPT-2)\n",
        "# Resultado: lista de números que representam o texto no vocabulário do modelo\n"
      ],
      "metadata": {
        "id": "OpABqg8fUmrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token IDs:\", input_ids)\n",
        "## Depois de gerar os input_ids, vamos imprimir para ver o resultado.\n",
        "# Cada número dessa lista representa um token (subpalavra) do texto no vocabulário do modelo.\n",
        "# Exemplo: [50256, 4186, 2216, 1234, ...]\n",
        "# - 50256 → token especial de início/fim usado pelo GPT-2\n",
        "# - outros números → IDs das subpalavras que formam a frase original\n",
        "# pode ser quebrado em diversas maneiras não so em palavras ceritnhas igual função split\n"
      ],
      "metadata": {
        "id": "huanJSYbUmtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte cada ID de volta para o token correspondente\n",
        "raw_tokens = [tokenizer.decode([token_id]) for token_id in input_ids]\n",
        "print(\"Raw tokens:\", raw_tokens)\n",
        "print(\"Numero de tokens:\", len(raw_tokens))\n",
        "\n",
        "# Converte cada ID individualmente de volta para o token original (em texto)\n",
        "# Isso ajuda a visualizar como o modelo quebrou a frase.\n",
        "# Obs: Tokens nem sempre são palavras inteiras; podem ser pedaços ou até caracteres.\n",
        "# Ex: \"INTELIGÊNCIA\" pode virar ['IN', 'TEL', 'IG', 'ÊNCIA'] dependendo do vocabulário.\n"
      ],
      "metadata": {
        "id": "CtXA_OPdUmvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_tensor = torch.tensor([input_ids])\n",
        "input_ids_tensor\n",
        "# Converte a lista de IDs para um tensor do PyTorch\n",
        "# [input_ids] → adiciona dimensão de \"batch\" (número de sequências processadas ao mesmo tempo)\n",
        "# Resultado: tensor com formato (1, seq_len)\n",
        "# - 1 = tamanho do batch (aqui só 1 frase)\n",
        "# - seq_len = número de tokens na frase"
      ],
      "metadata": {
        "id": "kdVBvqi1TyMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passa os IDs tokenizados pelo modelo BERT\n",
        "# - torch.no_grad(): desativa cálculo de gradiente (mais rápido e economiza memória)\n",
        "# - model(input_ids_tensor): executa o BERT em modo de inferência\n",
        "# Saída (outputs):\n",
        "#   outputs.last_hidden_state → tensor com embeddings de cada token (formato: [batch, seq_len, hidden_size])\n",
        "#     hidden_size = 768 para o BERT-base\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids_tensor)\n"
      ],
      "metadata": {
        "id": "yfhO6G3WTyO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrair embeddings\n",
        "embeddings = outputs.last_hidden_state\n",
        "print(embeddings.shape)  # (batch, seq_len, hidden_size)"
      ],
      "metadata": {
        "id": "lMmFUlb1TyRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Primeiro token da sequência (posição 0)\n",
        "cls_embedding = embeddings[0, 0]  # shape: (768,)  # pegou o primeiro vetor o primeiro embading olha quantos numeros deu para esse tokken\n",
        "print(cls_embedding)\n",
        "print(cls_embedding.shape)\n"
      ],
      "metadata": {
        "id": "OxWokaCy4peH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "king_token_id = tokenizer.convert_tokens_to_ids([\"king\"])[0]\n",
        "king_embedding = model.embeddings.word_embeddings(torch.tensor([king_token_id]))\n",
        "\n",
        "queen_token_id = tokenizer.convert_tokens_to_ids([\"queen\"])[0]\n",
        "queen_embedding = model.embeddings.word_embeddings(torch.tensor([queen_token_id]))\n",
        "\n",
        "cos = torch.nn.CosineSimilarity(dim=1)\n",
        "similarity = cos(king_embedding, queen_embedding)\n",
        "print(similarity[0])\n"
      ],
      "metadata": {
        "id": "f2wfQAgG8ICJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Similaridade cosseno (tabela de embeddings do BERT EN) ---\n",
        "# Observação: em 'bert-base-uncased' SEMPRE use minúsculas para lookup direto.\n",
        "def get_table_embedding_en(token_str: str):\n",
        "    tid = tokenizer.convert_tokens_to_ids(token_str)  # int (ID no vocabulário)\n",
        "    return model.embeddings.word_embeddings(torch.tensor([tid]))  # shape: (1, hidden=768)\n",
        "\n",
        "cos = torch.nn.CosineSimilarity(dim=1)\n",
        "\n",
        "pairs = [(\"king\", \"queen\"), (\"cat\", \"dog\"), (\"house\", \"queen\")]\n",
        "for a, b in pairs:\n",
        "    a_emb = get_table_embedding_en(a)\n",
        "    b_emb = get_table_embedding_en(b)\n",
        "    sim = float(cos(a_emb, b_emb)[0])\n",
        "    print(f\"cos({a},{b}) = {sim:.4f}\")\n",
        "\n",
        "    # ================================================================\n",
        "# O QUE ESTE BLOCO FAZ (explicação humana, sem mistério):\n",
        "#\n",
        "# Ideia: perguntar ao BERT se duas palavras “se parecem” no sentido.\n",
        "# Como? Pegamos, no “dicionário interno” do BERT (vocabulário),\n",
        "# o vetor que representa cada palavra (embedding fixo da tabela),\n",
        "# e comparamos esses vetores usando similaridade cosseno.\n",
        "#\n",
        "# Por que isso é útil?\n",
        "# - Mostra que o modelo tem um “mapa” de significados: palavras\n",
        "#   relacionadas ficam mais próximas (cosseno alto), e palavras\n",
        "#   sem relação ficam mais longe (cosseno baixo).\n",
        "#\n",
        "# Passo a passo do que o código faz:\n",
        "# 1) Pega o ID da palavra no vocabulário do BERT (ex.: \"king\" -> 4832).\n",
        "# 2) Usa esse ID para buscar o vetor da palavra na tabela de embeddings\n",
        "#    (um vetor de 768 números no BERT-base).\n",
        "# 3) Faz a mesma coisa para outra palavra (ex.: \"queen\").\n",
        "# 4) Compara os dois vetores com a similaridade cosseno:\n",
        "#       1.0  = muito parecidos (mesma direção)\n",
        "#       0.0  = nada a ver (90 graus, ortogonais)\n",
        "#      -1.0  = opostos (direções contrárias — raro nesses embeddings)\n",
        "# 5) Imprime o quão “próximas” as palavras são no espaço do modelo.\n",
        "#\n",
        "# Observações importantes:\n",
        "# - Aqui comparamos os VETORES DA TABELA (sem contexto da frase).\n",
        "#   Se quiser o “significado no contexto”, use outputs.last_hidden_state.\n",
        "# - Em 'bert-base-uncased', use tokens minúsculos no lookup direto.\n",
        "# - Valores baixos e positivos (ex.: 0.27) significam “distantes”, mas\n",
        "#   não necessariamente “opostos”; por isso nem sempre veremos números\n",
        "#   negativos.\n",
        "#\n",
        "# Exemplos típicos:\n",
        "# - cos(\"king\",\"queen\")  ~ 0.6+  → conceitos relacionados\n",
        "# - cos(\"cat\",\"dog\")     ~ 0.5   → ambos animais, relação moderada\n",
        "# - cos(\"house\",\"queen\") ~ 0.2-0.3 → pouca relação semântica\n",
        "#\n",
        "# Erros comuns e dicas:\n",
        "# - Misturar tokenizadores/modelos (ex.: IDs do GPT-2 no BERT) → IndexError.\n",
        "# - Usar \"House\" (maiúscula) no BERT uncased no lookup direto → pode virar [UNK].\n",
        "# - Rodar células fora de ordem e perder variáveis → NameError.\n",
        "#\n",
        "# Dica extra para portfólio:\n",
        "# - Mostre também 1 exemplo “contextual”: pegue embeddings de\n",
        "#   outputs.last_hidden_state para um token da sua frase e compare\n",
        "#   como o vetor muda em frases diferentes. Isso prova entendimento\n",
        "#   de “contextualidade”.\n",
        "# ==============================\n"
      ],
      "metadata": {
        "id": "0I1uyFsN8TTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SIMILARIDADE EM PORTUGUÊS"
      ],
      "metadata": {
        "id": "NVL3g5N4-T8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Similaridade cosseno em PT-BR usando BERT português\n",
        "#\n",
        "# Aqui vamos repetir a mesma lógica da etapa anterior (EN),\n",
        "# mas trocando o modelo/tokenizador para um treinado em português.\n",
        "#\n",
        "# Por que isso importa?\n",
        "# - O BERT inglês não tem no vocabulário as palavras \"rei\" e \"rainha\"\n",
        "#   (ou teria só como [UNK], token desconhecido).\n",
        "# - O modelo português já foi treinado com essas palavras, então\n",
        "#   conhece seus vetores reais no vocabulário e pode compará-los.\n",
        "# ================================================================\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Carregar modelo e tokenizador em português\n",
        "tok_pt = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "model_pt = AutoModel.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "model_pt.eval()\n",
        "\n",
        "# Função para buscar vetor de um token no vocabulário PT\n",
        "def get_table_embedding_pt(token_str: str):\n",
        "    tid = tok_pt.convert_tokens_to_ids(token_str)  # ID da palavra no vocabulário PT\n",
        "    return model_pt.embeddings.word_embeddings(torch.tensor([tid]))\n",
        "\n",
        "# Criar função de similaridade\n",
        "cos = torch.nn.CosineSimilarity(dim=1)\n",
        "\n",
        "# Embeddings fixos da tabela\n",
        "rei_emb = get_table_embedding_pt(\"rei\")\n",
        "rainha_emb = get_table_embedding_pt(\"rainha\")\n",
        "\n",
        "# Calcular similaridade\n",
        "sim_pt = float(cos(rei_emb, rainha_emb)[0])\n",
        "\n",
        "print(f\"[PT] cos(rei, rainha) = {sim_pt:.4f}\")\n"
      ],
      "metadata": {
        "id": "Xbu8Mufr-Sbo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}