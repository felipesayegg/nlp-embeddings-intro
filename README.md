# nlp-embeddings-intro

# ğŸ§  Primeiros Passos em NLP: TokenizaÃ§Ã£o e Word Embeddings

Este projeto Ã© meu **primeiro mergulho prÃ¡tico** no mundo de **Processamento de Linguagem Natural (NLP)**.  
Aqui, eu quis entender **na prÃ¡tica** como modelos como GPT-2 e BERT lidam com texto:  
- Como eles **quebram** uma frase em partes menores (tokens)
- Como transformam essas partes em **nÃºmeros** (IDs)
- E como esses nÃºmeros viram **vetores de significado** (embeddings)

No final, tambÃ©m explorei **como medir a â€œproximidadeâ€** entre palavras no espaÃ§o do modelo usando **similaridade cosseno**.

---

## ğŸš€ O que eu aprendi e pratiquei

1. **TokenizaÃ§Ã£o com GPT-2**
   - Usei o tokenizador do GPT-2 para ver como ele divide um texto em tokens.
   - Descobri que tokens nem sempre sÃ£o palavras inteiras â€” podem ser pedaÃ§os!
   - Comparei **quantidade de palavras** vs. **quantidade de tokens**.

2. **Embeddings com BERT (InglÃªs e PortuguÃªs)**
   - Carreguei o modelo BERT para inglÃªs (`bert-base-uncased`) e para portuguÃªs (`neuralmind/bert-base-portuguese-cased`).
   - Gerei embeddings para tokens e entendi a diferenÃ§a entre:
     - **Embeddings fixos** (da tabela de vocabulÃ¡rio)
     - **Embeddings contextuais** (que mudam conforme a frase)

3. **Similaridade Cosseno**
   - Comparei pares de palavras como `king` vs `queen`, `cat` vs `dog`, `rei` vs `rainha`.
   - Entendi que valores altos (prÃ³ximos de 1) significam que o modelo vÃª as palavras como **semÃ¢nticamente prÃ³ximas**.

4. **DiferenÃ§a entre Modelos**
   - Percebi que cada modelo tem seu **prÃ³prio vocabulÃ¡rio** e **tokenizaÃ§Ã£o**.
   - Misturar tokenizadores de modelos diferentes causa erro ou resultados falsos.

---

## ğŸ“š Conceitos que ficaram claros para mim

- **Biblioteca, mÃ³dulo, classe, mÃ©todo**  
  Como importar sÃ³ o que eu preciso (`from transformers import ...`) e criar objetos a partir de classes usando `.from_pretrained()`.

- **TokenizaÃ§Ã£o**  
  Quebrar texto em partes menores para que o modelo consiga transformar em nÃºmeros.

- **Embeddings**  
  Vetores que representam significado.  
  - Fixos: mesmo vetor sempre para o mesmo token.
  - Contextuais: vetor depende da frase.

- **Similaridade Cosseno**  
  Mede o quanto dois vetores â€œapontamâ€ para a mesma direÃ§Ã£o (1 = muito parecidos, 0 = nada a ver).

---

## ğŸ›  Tecnologias usadas
- [Python 3](https://www.python.org/)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)
- [PyTorch](https://pytorch.org/)

---

## ğŸ“‚ Estrutura do projeto
- `notebook.ipynb` â†’ cÃ³digo principal (limpo para renderizar no GitHub)
- `aula2_nlp_token_embedding.py` â†’ versÃ£o em script
- `requirements.txt` â†’ bibliotecas necessÃ¡rias

---

## ğŸ’¡ PrÃ³ximos passos
- Experimentar embeddings contextuais na comparaÃ§Ã£o de palavras.
- Comparar frases inteiras usando pooling de embeddings.
- Criar visualizaÃ§Ãµes grÃ¡ficas dos vetores com PCA/t-SNE.

---

âœï¸ **Autor:** Seu Nome Aqui  
ğŸ’¬ Se quiser trocar ideia sobre NLP ou modelos prÃ©-treinados, Ã© sÃ³ me chamar!
